apiVersion: chainsaw.kyverno.io/v1alpha1
kind: Test
metadata:
  name: evaluation-baseline
  labels:
    evaluated: "true"
spec:
  skip: true
  description: Test baseline evaluation functionality with ConfigMap-based golden examples
  steps:
  - name: step-1
    try:
    - script:
        skipLogOutput: true
        content: |
          set -u
          echo "{\"token\": \"$E2E_TEST_AZURE_OPENAI_KEY\", \"url\": \"$E2E_TEST_AZURE_OPENAI_BASE_URL\"}"
        outputs:
        - name: azure
          value: (json_parse($stdout))
    - script:
        content: |
          helm install ark-tenant ../../charts/ark-tenant --namespace $NAMESPACE --create-namespace --wait
        env:
        - name: NAMESPACE
          value: ($namespace)
    - apply:
        file: manifests/*.yaml
    - assert:
        resource:
          apiVersion: ark.mckinsey.com/v1alpha1
          kind: Model
          metadata:
            name: test-baseline-model
          status:
            conditions:
            - type: ModelAvailable
              status: "True"
    - assert:
        resource:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: golden-examples
    - wait:
        apiVersion: ark.mckinsey.com/v1alpha1
        kind: Evaluator
        timeout: 3m
        name: test-baseline-evaluator
        for:
          jsonPath:
            path: '{.status.phase}'
            value: 'ready'
    - wait:
        apiVersion: ark.mckinsey.com/v1alpha1
        kind: Evaluation
        timeout: 7m
        name: test-baseline-evaluation
        for:
          jsonPath:
            path: '{.status.phase}'
            value: 'done'
    - assert:
        resource:
          apiVersion: ark.mckinsey.com/v1alpha1
          kind: Evaluation
          metadata:
            name: test-baseline-evaluation
          status:
            (conditions[?type == 'Completed']):
            - status: 'True'
    - assert:
        resource:
          apiVersion: ark.mckinsey.com/v1alpha1
          kind: Evaluation
          metadata:
            name: test-baseline-evaluation
          status:
            phase: done
            (type(score) == 'string'): true
            (to_number(score) >= `0.0` && to_number(score) <= `1.0`): true
            (type(passed) == 'boolean'): true
    # Verify baseline evaluation metadata annotations
    - assert:
        resource:
          apiVersion: ark.mckinsey.com/v1alpha1
          kind: Evaluation
          metadata:
            name: test-baseline-evaluation
            # Check for evaluation metadata annotations with the correct prefix
            (contains(keys(annotations), 'evaluation.metadata/total_examples')): true
            (contains(keys(annotations), 'evaluation.metadata/passed_examples')): true
            (contains(keys(annotations), 'evaluation.metadata/failed_examples')): true
            (contains(keys(annotations), 'evaluation.metadata/average_score')): true
            # Check for category breakdown
            (contains(keys(annotations), 'evaluation.metadata/category_arithmetic_count')): true
            (contains(keys(annotations), 'evaluation.metadata/category_arithmetic_passed')): true
            # Check for difficulty breakdown
            (contains(keys(annotations), 'evaluation.metadata/difficulty_easy_count')): true
            (contains(keys(annotations), 'evaluation.metadata/difficulty_easy_passed')): true
    # Verify specific annotation values make sense
    - assert:
        resource:
          apiVersion: ark.mckinsey.com/v1alpha1
          kind: Evaluation
          metadata:
            name: test-baseline-evaluation
            # Verify numeric annotations have valid values
            (to_number(annotations."evaluation.metadata/total_examples") >= `1`): true
            (to_number(annotations."evaluation.metadata/passed_examples") >= `0`): true
            (to_number(annotations."evaluation.metadata/failed_examples") == `0`): true
            (to_number(annotations."evaluation.metadata/average_score") >= `0.0`): true
            (to_number(annotations."evaluation.metadata/average_score") <= `1.0`): true
            # Verify category counts
            (to_number(annotations."evaluation.metadata/category_arithmetic_count") >= `1`): true
            (to_number(annotations."evaluation.metadata/category_arithmetic_passed") >= `0`): true
            # Verify difficulty counts
            (to_number(annotations."evaluation.metadata/difficulty_easy_count") >= `1`): true
            (to_number(annotations."evaluation.metadata/difficulty_easy_passed") >= `0`): true
    catch:
    - events: {}
    - describe:
        apiVersion: ark.mckinsey.com/v1alpha1
        kind: Evaluation
        name: test-baseline-evaluation