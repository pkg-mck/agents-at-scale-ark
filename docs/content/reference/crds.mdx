---
title: Custom Resource Definitions (CRDs)
description: Detailed specifications for ARK's custom resources
---

# Custom Resource Definitions (CRDs)

This page provides detailed specifications for each ARK custom resource. For an overview of how these resources work with Kubernetes, see [Kubernetes Integration](/concepts/kubernetes).

## Resource Reference

| Resource | API Version | Description |
|----------|-------------|-------------|
| [Agent](/reference/resources/agent) | `ark.mckinsey.com/v1alpha1` | AI agents with prompts and tools |
| [Team](#teams) | `ark.mckinsey.com/v1alpha1` | Teams of agents with execution strategies |
| [Model](#models) | `ark.mckinsey.com/v1alpha1` | LLM service configurations |
| [Query](#queries) | `ark.mckinsey.com/v1alpha1` | Queries to agents or teams |
| [Tool](/reference/resources/tools) | `ark.mckinsey.com/v1alpha1` | Custom tools for agents |
| [MCPServer](#mcp-servers) | `ark.mckinsey.com/v1alpha1` | Model Context Protocol servers |
| [Evaluator](#evaluators) | `ark.mckinsey.com/v1alpha1` | AI-powered query assessment services |
| [Evaluation](#evaluations) | `ark.mckinsey.com/v1alpha1` | Multi-type AI output assessments |
| [A2AServer](#a2a-servers) | `ark.mckinsey.com/v1prealpha1` | Agent-to-Agent protocol servers |
| [ExecutionEngine](#execution-engines) | `ark.mckinsey.com/v1prealpha1` | External execution engines |

## Models

Models define connections to AI model providers and handle authentication and configuration.

### Specification
```yaml
apiVersion: ark.mckinsey.com/v1alpha1
kind: Model
metadata:
  name: default  # Special name 'default' used when agents don't specify a model
spec:
  type: azure  # openai, azure, bedrock
  model:
    value: gpt-4.1-mini
  config:
    azure:
      baseUrl:
        value: "https://lxo.openai.azure.com"
      apiKey:
        valueFrom:
          secretKeyRef:
            name: default-model-token
            key: token
      apiVersion:
        value: "2024-12-01-preview"
```

### Supported Providers
- **Azure OpenAI**: Enterprise-grade OpenAI models
- **OpenAI**: Direct OpenAI API access
- **AWS Bedrock**: Amazon's managed AI service
- **Gemini**: Google's AI models

### Configuration Options
- **API Keys**: Stored securely in Kubernetes secrets
- **Base URLs**: Custom endpoints for different providers
- **API Versions**: Provider-specific API versions
- **Model Parameters**: Temperature, max tokens, etc.

## Teams

Teams coordinate multiple agents working together using different execution strategies.

### Specification
```yaml
apiVersion: ark.mckinsey.com/v1alpha1
kind: Team
metadata:
  name: team-seq
spec:
  members:
    - name: agent-seq
      type: agent
    - name: agent-seq
      type: agent
    - name: agent-seq
      type: agent
  strategy: "sequential"
```

### Execution Strategies
- **sequential**: Agents process input one after another
- **parallel**: Agents process input simultaneously
- **round-robin**: Agents take turns processing inputs
- **selector**: Dynamic agent selection based on criteria

### Member Types
- **agent**: Reference to an Agent resource
- **team**: Reference to another Team resource (nested teams)

### Advanced Features
Teams support complex workflows including:
- **Graph-based strategies**: Custom execution flows
- **Conditional routing**: Dynamic member selection
- **Termination conditions**: Early completion criteria

## Queries

Queries represent requests sent to agents or teams and track their execution and results.

### Specification
```yaml
apiVersion: ark.mckinsey.com/v1alpha1
kind: Query
metadata:
  name: weather-query
spec:
  input: 'What is the weather today in New York?'
  targets:
    - type: agent
      name: weather
```

### Target Types
- **agent**: Send query to a specific agent
- **team**: Send query to a team of agents
- **selector**: Use label selectors to choose targets dynamically

### Advanced Features
- **Session Management**: Group related queries with sessionId
- **Template Parameters**: Use variables in query input
- **Timeout Control**: Set maximum execution time
- **Multiple Targets**: Send same query to multiple agents/teams
- **Evaluators**: Automatic assessment of query results




## Evaluators

Evaluators provide either deterministic or AI-powered assessment of teams / agents / queries / tools to support quality control and testing.
For non-deterministic cases, they use the "LLM-as-a-Judge" pattern to automatically evaluate agent responses.

### How Evaluators Work

- **Optional Integration**: Queries can optionally reference an Evaluator for quality assessment
- **LLM-as-a-Judge**: Evaluators use AI models to assess response quality across multiple criteria
- **Automatic Triggering**: When a query completes, if an evaluator is specified, the query enters "evaluating" phase
- **Quality Gating**: Only after successful evaluation is the query marked as "done"

### Specification
```yaml
apiVersion: ark.mckinsey.com/v1alpha1
kind: Evaluator
metadata:
  name: ark-evaluator
spec:
  description: "evaluator for query assessment"
  address:
    valueFrom:
      serviceRef:
        name: ark-evaluator
        port: "http"
        path: "/evaluate"
  selector:  # optional - for automatic query evaluation
    resourceType: Query
    matchLabels:
      evaluate: "true" # will automatically evaluate any query with label evaluate=true
  parameters:  # optional - including model configuration
    - name: model.name      # specify model to use (default: "default")
      value: "gpt-4-model"
    - name: model.namespace # specify model namespace (default: evaluator's namespace)
      value: "models"
    - name: min-score      # custom parameter passed to the evaluation service
      value: "0.8"
```

**Auto-triggered Evaluation:**
```
┌─────────────────────┐                    ┌──────────────┐
│ Evaluator with      │     Auto-triggers  │ New Query    │
│ selector:           │ ◄───────────────── │ labels:      │
│   matchLabels:      │     when created   │   evaluate:  │
│     evaluate: true  │     or modified    │     "true"   │
└─────────────────────┘                    └──────────────┘
```

### Key fields
- **Address - Service Reference**: Target specific execution service to execute an evaluation.
- **Query selector**: Automatic match of queries to be evaluated using specific labels. 
- **Parameter map**: Pass default evaluation parameters to the target evaluation service.

## Evaluations

Evaluations assess different metrics using various modes including direct assessment, dataset comparison, and query result evaluation.  
Current design is aligned with: One Evaluation = One Evaluator = One Specific Assessment

### Overview

Evaluations work with Evaluators to assess AI outputs, both deterministic and non-deterministic. 
Multiple evaluation modes can target different evaluators, and evaluators can automatically process evaluations based on label selectors.

**Evaluation Flow:**
```
┌─────────────┐      ┌─────────────┐      ┌─────────────────┐
│ Evaluation  │◄─────│ Evaluator(s)│      │ Ark Evaluation  │
│  Mode       │─────►│             │─────►│ Service(s)      │
└─────────────┘      └─────────────┘      └─────────────────┘
       │                     
       └─────► (*) Query ────► (Agent/Tool/Team)
```

### Key Fields
- **type**: Evaluation type (direct, baseline, query, batch, event)
- **evaluator**: Reference to the Evaluator resource
- **config**: Type-specific configuration with embedded fields
- **status.score**: Evaluation score (0-1)
- **status.passed**: Whether evaluation passed

### Evaluation Types

#### Direct Type

Evaluate a single input/output pair:
```yaml
apiVersion: ark.mckinsey.com/v1alpha1
kind: Evaluation
metadata:
  name: direct-eval
spec:
  type: direct
  evaluator:
    name: quality-evaluator
  config:
    input: "What's the weather in NYC?"
    output: "It's 72°F and sunny in New York City"
```

#### Baseline type
Evaluate against baseline datasets to measure performance and verify that the evaluator achieves proper metrics:

```yaml
apiVersion: ark.mckinsey.com/v1alpha1
kind: Evaluation
metadata:
  name: baseline-eval
spec:
  type: baseline
  timeout: "10m"  # Extended timeout to keep the controller connection open until all samples get processed
  evaluator:
    name: llm-judge
    parameters:
    - name: golden-examples # reference dataset with test cases to baseline the evaluator performance
      valueFrom:
        configMapKeyRef:
          name: golden-examples
          key: examples
  config: {}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: golden-examples
data:
  examples: |
    [
      {
        "input": "What is 7 + 3?",
        "expectedOutput": "10",
        "expectedMinScore": "0.9",
        "difficulty": "easy",
        "category": "arithmetic",
        "metadata": {
          "type": "basic-addition",
          "concept": "addition"
        }
      },
      {
        "input": "What is 6 × 4?",
        "expectedOutput": "24",
        "expectedMinScore": "0.85",
        "difficulty": "easy",
        "category": "arithmetic",
        "metadata": {
          "type": "multiplication",
          "concept": "multiplication"
        }
      },
      {
        "input": "If I buy 8 items at $3 each, how much do I spend?",
        "expectedOutput": "$24",
        "expectedMinScore": "0.8",
        "difficulty": "medium",
        "category": "word-problem",
        "metadata": {
          "type": "word-problem",
          "concept": "multiplication",
          "context": "shopping"
        }
      },
      {
        "input": "What is 1/2 + 1/4?",
        "expectedOutput": "3/4",
        "expectedMinScore": "0.75",
        "difficulty": "hard",
        "category": "fractions",
        "metadata": {
          "type": "fraction-addition",
          "concept": "fractions"
        }
      }
    ]
```

#### Query type evaluation

Evaluate existing query results:

```yaml
apiVersion: ark.mckinsey.com/v1alpha1
kind: Evaluation
metadata:
  name: query-eval
spec:
  type: query
  evaluator:
    name: accuracy-evaluator
  config:
    queryRef:
      name: weather-query-123
      responseTarget: "weather-agent"  
```

#### Batch type

```
┌─────────────┐      
│ Evaluation  │      Aggregates multiple child evaluations
│ type=Batch  │ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ─ ─ ─ ─ ─┐
└─────────────┘                                   │
                                                  ▼
                          ┌─────────────┐  ┌─────────────┐  ┌─────────────┐
                          │ Eval 1      │  │ Eval 2      │  │ Eval n      │
                          │ type=Query  │  │ type=Query  │  │ type=Direct │
                          └─────────────┘  └─────────────┘  └─────────────┘
```

Example combining explicit items with template-based dynamic creation
```yaml

apiVersion: ark.mckinsey.com/v1alpha1
kind: Evaluation
metadata:
  name: hybrid-batch-eval
  namespace: default
spec:
  type: batch
  config:
    # Explicit evaluations (high priority, specific configs)
    items:
      - name: critical-accuracy-test
        type: direct
        evaluator:
          name: strict-evaluator
          parameters:
            - name: threshold
              value: "0.95"
        config:
          input: "What is the capital of France?"
          output: "Paris"
      
      - name: performance-baseline
        type: query
        evaluator:
          name: performance-evaluator
        config:
          queryRef:
            name: baseline-query
    
    # Template for dynamic creation from query selector
    template:
      namePrefix: auto-eval
      evaluator:
        name: standard-evaluator
      type: query
      config:
        queryRef:
          name: ""  # Will be filled dynamically
    
    # Select additional queries to evaluate using the template
    querySelector:
      matchLabels:
        category: "regression-test"
        priority: "medium"
      matchExpressions:
        - key: status
          operator: In
          values: ["completed", "ready"]
    
    concurrency: 3
    continueOnFailure: true
```

#### Event/Rule based evaluation

Rule-based evaluations using CEL (Common Expression Language):
```yaml
apiVersion: ark.mckinsey.com/v1alpha1
kind: Evaluation
metadata:
  name: event-eval
spec:
  type: event
  evaluator:
    name: tool-usage-evaluator
  config:
    rules:
      - name: "weather-tool-called"
        expression: 'tools.was_called('get-weather')'
        description: "Validates get-weather tool was called"
        weight: 1
```

### Using Evaluators in Queries (DEPRECATED)

Reference an evaluator in your query to enable automatic assessment:

```yaml
apiVersion: ark.mckinsey.com/v1alpha1
kind: Query
metadata:
  name: research-query
spec:
  input: "Analyze renewable energy trends"
  targets:
    - type: agent
      name: research-agent
  evaluator:
    name: llm-evaluator
```

### Evaluation Process

1. **Query Execution**: Agent processes the query and generates response
2. **Evaluation Trigger**: Query enters "evaluating" phase
3. **Assessment**: Evaluator analyzes response quality using configured criteria
4. **Scoring**: Evaluator provides numerical score and qualitative feedback
5. **Completion**: Query marked as "done" with evaluation results

### Evaluation Results

Evaluation results are stored in the Query status:

```yaml
status:
  phase: done
  evaluations:
  - evaluatorName: llm-evaluator
    passed: true
    score: "0.85"
    metadata:
      reasoning: "Response provides comprehensive analysis with supporting data"
      criteria:
        accuracy: "high"
        completeness: "good"
        relevance: "excellent"
```

## A2A Servers

A2A (Agent-to-Agent) Servers enable hosting external agent frameworks within ARK.

### Specification
```yaml
apiVersion: ark.mckinsey.com/v1prealpha1
kind: A2AServer
metadata:
  name: langchain-agents
spec:
  address:
    valueFrom:
      serviceRef:
        name: langchain-service
        port: "8080"
```

## Execution Engines

Execution Engines provide custom runtime environments for specialized agent execution.

### Specification
```yaml
apiVersion: ark.mckinsey.com/v1prealpha1
kind: ExecutionEngine
metadata:
  name: custom-engine
spec:
  type: external
  endpoint: "http://custom-engine-service:8080"
```

## Resource Relationships

ARK resources work together in common patterns:

- **Agent + Model + Tools**: Basic agent with capabilities
- **Team + Multiple Agents**: Multi-agent collaboration
- **Query + Targets**: Requests to agents or teams
- **MCP Server + Tools**: Standardized tool integration
- **Memory + Sessions**: Persistent conversations

---

**Next**: Learn about [CLI Tools](/concepts/cli-tools) for working with these resources.
